---
title: Gated Recurrent Unit ðŸŸ¥
usemathjax: true
---

Recurrent neural network unit with a gating mechanism

- Similar to [[LSTM]]
	- Adds a forget gat
	- Removes an ouput gate 
- Fewer parameters than [[LSTM]]


Why and when would you want to use a GRU over an LSTM?


# Architecture




