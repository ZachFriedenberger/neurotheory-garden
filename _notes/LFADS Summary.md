---
title: Latent Factor Analysis via Dynamical Systems ðŸŸ¥
usemathjax: true
---

> **tl;dr:**  This paper introduces a method for infering latent dynamics from simultaneously recorded single-trial high-dimensional neural spiking data.

>**Citation:** @article{DBLP:journals/corr/SussilloJAP16,
  author    = {David Sussillo and
               Rafal J{\'{o}}zefowicz and
               L. F. Abbott and
               Chethan Pandarinath},
  title     = { LFADS - Latent Factor Analysis via Dynamical Systems},
  journal   = {CoRR},
  volume    = {abs/1608.06315},
  year      = {2016},
  url       = {http://arxiv.org/abs/1608.06315},
  eprinttype = {arXiv},
  eprint    = {1608.06315},
  timestamp = {Sat, 23 Jan 2021 01:20:31 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/SussilloJAP16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


**Paper link:** [https://arxiv.org/abs/1608.06315](https://arxiv.org/abs/1608.06315/)

---

# 1 Introduction
- More high-dimensional neural data is available (multiple single-units)
- Majority of experiemental neuroscientists still use single-unit PSTHs
- Two forms of computation in the brain 
	- Feed-forward processing (no temporal dyanmics + input driven)
	- Sequential processing (dynamical system flows from initial condition)
- LFADS hypothesizes that driven nonlinear dynamics provide a reasonable model of many neural processes
- Goal is to infer smooth single-unit dynamics from spike trains
- Provides a low-dimensional set of temporal factors explaining the data
- Provides a set of initial conditions that code each trial 
- Infers inputs 
	- Uses a form of surpirse. If nonlinear dynamical system can't generate the data, then it was caused by an external perturbation



# 2 The LFADS Model

- LFADS is a variational autoencoder ([[Auto-Encoding Variational Bayes Summary|VAE]]) extened to sequenes
- Let $\mathbf{v} = W(\mathbf{u})$ be an affine transformation, $[\cdot, \cdot]$ is vector concatenation, and $s_{t} = \text{RNN}(s_{t-1}, \text{input}_{t})$ be a state update for an $\text{RNN}$. 


## 2.1 LFADS Generator

- Consider neural spike trains $\mathbf{x}_{1:T}$ from $D$ neurons
- Each $\mathbf{x}_{1:T}$ instance is a trial 
- May include other observables or stimuli $\mathbf{a}_{1:T}$ to inform the modelling, but it is not being modelled directly
- The generator $p(\mathbf{x}\vert \mathbf{z})$ is only over $\mathbf{x}$, but the approximate posterior $q(\mathbf{z}\vert \mathbf{x}, \mathbf{a})$ depends on both data types
- We assume $\mathbf{x_{1:T}}$ comes from a Poisson process with underlying rates $\mathbf{r_{1:T}}$
- Goal is to recover low-dimensional latent dynamics $\mathbf{f}_{1:T}$ from which $\mathbf{r}_{1:T}$ can be constructed
- Rates are determined by, $\mathbf{r}_{1:T} = \text{exp}(W^{\text{rate}}(\mathbf{f}_{1:T}))$
- low-d representation is based on the fact that the intrinsict dimensionality of neural recordings tends to lower than the number of neurons
- factors are generated by a RNN characterized by, $\mathbf{f}_{1:T}=W^{\text{fac}}(\mathbf{g}_{1:T})$ with the initial condition $\mathbf{g}_{0}~p(\mathbf{g}_{0})$ drawn from a prior distribution
- latent variables are $z=\\{\mathbf{g}_{0}, \mathbf{u}_{1:T}\\}$, where $\mathbf{u}_{1:T}$ is the infered input and $\mathbf{u}_{t}$ is drawn from a prior at each time step $\mathbf{u}_{t} \sim p(\mathbf{u}_{t})$


1. Sample initial condition from prior, $\hat{\mathbf{g}}_{0} \sim p(\mathbf{g}_{0})$
2. At each time step $t=1,...,T$ an infered input $\mathbf{u}_{t}$ is sampled and fed into the network

$$\begin{align}\hat{\mathbf{u}}_{t} &\sim p(\mathbf{u}_{t}) \\ \mathbf{g}_{t} &= \text{RNN}^{\text{gen}}(\mathbf{g}_{t-1}, \hat{\mathbf{u}}_{t}) \\ \mathbf{f}_{t} &= W^{\text{fac}}(\mathbf{g}_{t}) \\ \mathbf{r}_{t} &= \text{exp}(W^{\text{rate}}(\mathbf{f}_{t})) \\ \hat{\mathbf{x}}_{t} &\sim \text{Poisson}(\mathbf{x}_{t} \vert \mathbf{r}_{t})\end{align}$$

- Each component of $\mathbf{x}_{t}$ is generated by an independant Poisson process with a rate given by the component of $\mathbf{r}_{t}$
- Priors are given by multivariate Gaussians with zero means and a diagonal covariance structure
- The RNN unit used is a [[GRU]]
- observed data $\mathbf{a}$ is not included in the generator model, but can be used as an additional input the RNN above
- This model implements the conditional distribution $p(\mathbf{x}\vert\mathbf{z}) = p(\mathbf{x}\vert \\{\mathbf{g}_{0}, \mathbf{u}_{1:T}\\})$ 




## 2.2 LFADS Encoder

- We have two distributions, $q(\mathbf{g}_{0} \vert \mathbf{x}, \mathbf{a})$ and $q(\mathbf{u}_{t} \vert \mathbf{x}, \mathbf{a})$


- Let $q(\mathbf{g_{0}\vert \mathbf{x,a}})$ be multivaraite Gaussian with mean and varaince given by the network outputs

$$\begin{align}\mathbf{\mu^{g_{0}}} &= W^{\mu^{g_{0}}}(\mathbf{E}) \\ \mathbf{\sigma^{g_{0}}} &= \text{exp}(\frac{1}{2}W^{\sigma^{g_{0}}}(\mathbf{E}))\end{align}$$

Where $\mathbf{E}$ comes from running a RNN forward and backward in time. 

$$\begin{align} e^{b}_{t} &= \text{RNN}^{\text{enc,b}}(e^{b}_{t+1}, [x_{t}, a_{t}]) \\ e^{f}_{t} &= \text{RNN}^{\text{enc,f}}(e^{f}_{t-1}, [x_{t}, a_{t}]) \\ \mathbf{E} &= [e_{1}^{b}, e_{T}^{f}]\end{align}$$
Where the initial conditions $e^{b}_{T+1}$ and $e^{f}_{0}$ are learnable parameters

- Allows the network ouput to reflect the entire time hsitroy of the data $x_{1:T}$ and $a_{1:T}$. WHY??

- Let $\tilde{\mathbf{E}}_{t} = [\tilde{e}_{t}^{b}, \tilde{e}_{t}^{f}]$ be a time-dependent equivalent for $\mathbf{u}_{t}$

- Introduce a RNN controller network with output $\mathbf{c}_{t}$

$$\mathbf{c}_{t} = \text{RNN}^{\text{con}}(\mathbf{c}_{t}, [\tilde{\mathbf{E}}_{t}, \mathbf{f}_{t-1}])$$
- Initial state is deterministic $\mathbf{c}_{0}=W^{c_{0}}(\mathbf{E})$


$$\begin{align}
\hat{\mathbf{u}}_{t} &\sim q(\mathbf{u}_{t} \vert \mu_{t}^{u},\sigma_{t}^{u}) \\ 
\mu_{t}^{u} &= W^{\mu^{u}}(\mathbf{c}_{t}) \\ 
\sigma_{t}^{u} &= \text{exp}\big(\frac{1}{2}W^{\sigma^{u}}(\mathbf{c}_{t})\big) 

\end{align}$$

- information flow is controlled by regularizer on $\mathbf{u}_{t}$ and by restricting the dimensionality as a hyperparameter


## 2.3 Full inference model 
- encoders are run on data trial
- initial conditions are sampled 
$$\begin{align}\mathbf{c}_{t} &= \text{RNN}^{\text{con}}(\mathbf{c}_{t}, [\tilde{\mathbf{E}}_{t}, \mathbf{f}_{t-1}]) \\ 
\mu_{t}^{u} &= W^{\mu^{u}}(\mathbf{c}_{t}) \\ 
\sigma_{t}^{u} &= \text{exp}\big(\frac{1}{2}W^{\sigma^{u}}(\mathbf{c}_{t})\big) 
 \\ 
 \hat{\mathbf{u}}_{t} &\sim q(\mathbf{u}_{t} \vert \mu_{t}^{u}, \sigma_{t}^{u}) \\ \mathbf{g}_{t} &= \text{RNN}^{\text{gen}}(\mathbf{g}_{t-1}, \hat{\mathbf{u}}_{t}) \\ \mathbf{f}_{t} &= W^{\text{fac}}(\mathbf{g}_{t}) \\ \mathbf{r}_{t} &= \text{exp}(W^{\text{rate}}(\mathbf{f}_{t})) \\ \hat{\mathbf{x}}_{t} &\sim \text{Poisson}(\mathbf{x}_{t} \vert \mathbf{r}_{t})\end{align}$$


## 2.4 The loss function




# 3 Relation to Previous Work
- Other VAE extensions to the recurrent setting
	- variational reccurent networks
	- deep Kalman filters
	- RNN DRAW network
- Research applying probabilisdtic sequential graphical models to neural data
	- PLDS
	- swithcing LDS
	- GCLDS
	- PfLDS

- Gaussian process models 
	- GPFA (infers time constant to smooth the neural data)
	- vLGP (variational approach)

- Deep Kalman Filter



# 4 Results
- Compared LFADS to GPFA, vLGP, and PfLSDS
- Generated syntetic stochsastic spike trains from two deterministic nonlinear systems
	- Lorenz system 
	- RNN with external input

## 4.3 Infering inputs to the data RNN
- Used L2 regularizer on the generator network to find simple solutions (low-dimensional)
- Doesn't look like it works as well in cases of more complex dynamics
- Seems like the dimension of the input is pre-specified? 
- Need to revist how they tested autonomous vs. non-autonomous assumption
	- Any driven system can be rewritten as the dynamics of a system with extra dimensions


# 5 Discussion


